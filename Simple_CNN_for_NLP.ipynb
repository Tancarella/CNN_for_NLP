{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "\n",
    "train_data = pd.read_csv(\n",
    "    \"data/training.csv\",\n",
    "    header= None,\n",
    "    names = cols,\n",
    "    engine = \"python\",\n",
    "    encoding = \"latin1\"\n",
    ")\n",
    "\n",
    "test_data = pd.read_csv(\n",
    "    \"data/test.csv\",\n",
    "    header= None,\n",
    "    names = cols,\n",
    "    engine = \"python\",\n",
    "    encoding = \"latin1\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
    "    axis = 1,\n",
    "    inplace = True)\n",
    "\n",
    "test_data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
    "    axis = 1,\n",
    "    inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_clean = [clean_tweet(tweet) for tweet in train_data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_labels = train_data.sentiment.values\n",
    "train_data_labels[train_data_labels == 4] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    train_data_clean, target_vocab_size = 2**16\n",
    ")\n",
    "\n",
    "train_data_inputs = [tokenizer.encode(sentence) for sentence in train_data_clean]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max([len(sentence) for sentence in train_data_inputs])\n",
    "train_data_inputs = tf.keras.preprocessing.sequence.pad_sequences(train_data_inputs,\n",
    "    value = 0,\n",
    "    padding = \"post\",\n",
    "    maxlen = MAX_LEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training/testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = np.random.randint(0, 800000, 8000)\n",
    "test_idk = np.concatenate((test_idx, test_idx + 800000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = train_data_inputs[test_idx]\n",
    "test_labels = train_data_labels[test_idx]\n",
    "train_inputs = np.delete(train_data_inputs, test_idx, axis = 0)\n",
    "train_labels = np.delete(train_data_labels, test_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, \n",
    "                vocab_size, \n",
    "                emb_dim = 128, \n",
    "                nb_filters = 50, \n",
    "                FFN_units = 512, \n",
    "                nb_classes = 2, \n",
    "                dropout_rate = 0.1,\n",
    "                training = False,\n",
    "                name = \"dcnn\"):\n",
    "        super(DCNN, self).__init__(name = name)\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, emb_dim)\n",
    "        self.bigram = layers.Conv1D(filters = nb_filters, kernel_size = 2, padding = \"valid\", activation = \"relu\")\n",
    "        self.pool_1 = layers.GlobalMaxPool1D()\n",
    "        self.trigram = layers.Conv1D(filters = nb_filters, kernel_size = 3, padding = \"valid\", activation = \"relu\")\n",
    "        self.pool_2 = layers.GlobalMaxPool1D()\n",
    "        self.fourgram = layers.Conv1D(filters = nb_filters, kernel_size = 4, padding = \"valid\", activation = \"relu\")\n",
    "        self.pool_3 = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units = FFN_units, activation = \"relu\")\n",
    "        self.dropout = layers.Dropout(rate = dropout_rate)\n",
    "\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units = 1, activation = \"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units = nb_classes, activation = \"softmax\")\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool_1(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool_2(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool_3(x_3)\n",
    "\n",
    "        merged = tf.concat([x_1, x_2, x_3], axis = -1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "EMB_DIM = 200\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = len(set(train_labels))\n",
    "\n",
    "DROPOUT_Rate = 0.2\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dcnn = DCNN(vocab_size = VOCAB_SIZE,\n",
    "    emb_dim = EMB_DIM,\n",
    "    nb_filters = NB_FILTERS,\n",
    "    FFN_units = FFN_UNITS,\n",
    "    nb_classes = NB_CLASSES,\n",
    "    dropout_rate = DROPOUT_Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss = \"binary_crossentropy\",\n",
    "        optimizer = \"adam\",\n",
    "        metrics = [\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "        optimizer = \"adam\",\n",
    "        metrics = [\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./ckpt/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Dcnn = Dcnn)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 1)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dcnn.fit(train_inputs, train_labels, batch_size = BATCH_SIZE, epochs = NB_EPOCHS)\n",
    "ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
